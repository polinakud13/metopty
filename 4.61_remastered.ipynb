{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem statement\n",
    "\n",
    "Let $X \\in \\{0,1\\}$ be a random variable, that satisfies $$prob(X=1) = p = \\frac {exp(a^{T}x+b)}{1+exp(a^{T}x+b)} $$  where $x \\in \\mathbb{R}^n$ is a vector of variables that affect the probability, and $a$ and $b$ are known parameters. We can think of $X = 1$ as the event that a consumer buys a product, and $x$ as a vector of variables that affect the probability, e.g., advertising effort, retail price, discounted price, packaging expense, and other factors. \n",
    "The variable $x$, which we are to optimize over, is subject to a set of linear constraints, $Fx \\preceq g$.\n",
    "\n",
    "Formulate the following problems as convex optimization problems.\n",
    "\n",
    "\n",
    "1)  _Maximizing buying probability_ The goal is to choose $x$ to maximize $p$.\n",
    "\n",
    "2)  _Maximizing expected profit._ Let $c^T x+d$ be the profit derived from selling the product, which we assume is positive for all feasible x. The goal is to maximize the expected profit, which is $p\\cdot(c^T x + d)$.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as sps\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_a(x, a=a, b=b):\n",
    "    return -a.T * x - b\n",
    "    \n",
    "def f_b(x, a= a, b= b,c=c, d=d):\n",
    "    return -(c.T*x + d) * np.exp(a.T*x + b) / (1. + np.exp(a.T*x + b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the optimization methods work for minimizing function f, in our task we minimize -f, which is the same as maximizing f. \n",
    "\n",
    "a) Gradient here is constant, $p' = -a$\n",
    "\n",
    "b) Gradient: $$p' = -a + \\frac {e^{a^Tx + b}}{1 + e^{a^Tx + b}} - \\frac {c}{c^Tx + d}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_a(x, a=a, **kwargs):\n",
    "    return -a\n",
    "def grad_b(x, a=a, b=b, c=c, d=d, **kwargs):\n",
    "    nom = np.exp(a.T*x + b)[0, 0]\n",
    "    denom = 1 + nom\n",
    "    \n",
    "    if c.T*x + d <= 0:\n",
    "        raise Exception('grad_b error: Divided by zero')\n",
    "    return -a + nom*a/denom - c/(c.T*x + d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Hesse matrix is zero, as derivative is constant.\n",
    "\n",
    "b) Hesse matrix: $$ \\frac{cc^T}{(c^Tx + d)^2}  + \\frac {e^{a^Tx + b}}{(1+e^{a^Tx + b})^2}aa^T$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hesse_a(x, **kwargs):\n",
    "    return 0\n",
    "def hesse_b(x, a=a, b=b, c=c, d=d, **kwargs):\n",
    "    nom = np.exp(a.T*x + b)[0,0]\n",
    "    \n",
    "    \n",
    "    if c.T*x + d <= 0:\n",
    "        raise Exception('grad_b error: Divided by zero')\n",
    "\n",
    "    return c*c.T/(c.T*x+d)**2 + nom/(nom+1)**2*a*a.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_descent(grad, start_x, F= None, g= None, stop_precision=0.0001, epsilon=0.001, **kwargs):\n",
    "    work_x = start_x\n",
    "    prev_x = 0\n",
    "    prev_precision = 1\n",
    "    iterat = 0\n",
    "    for i in range(10000):\n",
    "        eps = epsilon\n",
    "        iterat += 1\n",
    "        prev_x = work_x\n",
    "        \n",
    "        if (F is not None) and (g is not None):\n",
    "            it = 1\n",
    "            while np.count_nonzero(g - F*(work_x - eps*grad(prev_x,**kwargs)) <= 0) > 0: #projectioin\n",
    "                it += 1\n",
    "                eps = 0.6 * eps\n",
    "                if(it > 100000):\n",
    "                    return work_x\n",
    "        \n",
    "        work_x = work_x - eps * grad(prev_x,**kwargs)\n",
    "        prev_precision = np.max(np.absolute(prev_x - work_x))\n",
    "\n",
    "        if prev_precision < stop_precision:\n",
    "            break\n",
    "    return work_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def barrier_method(start_x,\n",
    "                   start_t,\n",
    "                   F, g,\n",
    "                   grad_func,\n",
    "                   hesse_func=None, \n",
    "                   optimization_method=grad_descent,\n",
    "                   mu= 1.05, \n",
    "                   eps = (10.0)**(-3),\n",
    "                   iterations_limit = 1000,\n",
    "                   **kwargs):\n",
    "    \"\"\"\n",
    "    Barrier method for minimizing f subject to Fx < g\n",
    "    Instead of minimizing f we will minimize \n",
    "    h_t = t * f + penalty_function,\n",
    "    where penalty_function is \n",
    "    - sum (from 1 to m) ln(g_i - f_i * x)\n",
    "    f_i - i-th row of matrix F, g_i - i-th value in g,\n",
    "    t - positive number\n",
    "    start_t is passed to input, later the number is changed \n",
    "    \n",
    "    Parameters:\n",
    "    start_x - np.matrix with shape (n, 1)\n",
    "    start_t - float\n",
    "    F - np.matrix with shape (m, n)\n",
    "    g - np.matrix with shape (m, 1)\n",
    "    grad_func - function, computing gradient for f, \n",
    "                returns np.matrix with shape (n, 1)\n",
    "                \n",
    "    hesse_func - function, computing hessian for f,\n",
    "                returns np.matrix with shape (n, n)\n",
    "    optimization_method - chosen method of finding minimum of \n",
    "                function (f + penalty_function)\n",
    "    mu - float, measure of parameter t increase\n",
    "    eps - precision of computing argmin f\n",
    "    \"\"\"\n",
    "    \n",
    "    def grad_log_func(x):\n",
    "        d = 1/(g - F * x)\n",
    "        return F.T * d\n",
    "    \n",
    "    def hesse_log_func(x):\n",
    "        d = 1/(g - F* x)\n",
    "        D = np.diag((np.array(d.T)).reshape(np.size(d)))\n",
    "        return F.T * D * D * F\n",
    "    \n",
    "    \n",
    "    m = np.size(g, axis= 1) # dimension of g\n",
    "    t = start_t\n",
    "    xx = start_x\n",
    "    iterations = 0\n",
    "    while iterations < iterations_limit:\n",
    "\n",
    "        def grad_x(x, **kwargs):\n",
    "            \"\"\"\n",
    "            compute grad of h_t(x)\n",
    "            \"\"\"\n",
    "            return t * grad_func(x, **kwargs) + grad_log_func(x)\n",
    "        \n",
    "        if hesse_func is None: \n",
    "            x_star = optimization_method(grad_x, xx, F= F, g=g, stop_precision=eps, **kwargs)\n",
    "        \n",
    "        if hesse_func is not None: # basically the case of Newton's method\n",
    "            def hesse_x(x, **kwargs):\n",
    "                \"\"\"\n",
    "                compute hessian of h_t(x)\n",
    "                \"\"\"\n",
    "                return t * hesse_func(x, **kwargs) + hesse_log_func(x)\n",
    "            \n",
    "            x_star = optimization_method(grad_x, hesse_x, xx, F= F, g=g, stop_precision=eps, **kwargs)\n",
    "        \n",
    "        xx = x_star\n",
    "        if m / t < eps:\n",
    "            break\n",
    "        else:\n",
    "            t = mu * t\n",
    "        iterations += 1\n",
    "    return xx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quasi-Newton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quasi_newton_method(grad, start_x, F= None, g=None, stop_precision=0.0001,\\\n",
    "                        step_size=1, iterations=10000, **kwargs):\n",
    "    \"\"\"\n",
    "    Minimizes function using quasi Newton method\n",
    "    grad - gradient of a function to minimize\n",
    "    start_x - starting point\n",
    "    \"\"\"\n",
    "    def update_inverse_hesse(prev, s, y):\n",
    "        v = s - prev*y\n",
    "        if v.T*y == 0 or v.T*y != v.T*y:\n",
    "             raise Exception('hesse error: Divided by zero %f' % v.T*y)\n",
    "        return prev + v*v.T/(v.T*y)\n",
    "    \n",
    "    curr_x = start_x\n",
    "\n",
    "    inverse_hesse = np.identity(start_x.size) #hesse aproximation\n",
    "\n",
    "    for i in range(iterations):\n",
    "        eps = step_size\n",
    "        \n",
    "        delta_x = -inverse_hesse * grad(curr_x, **kwargs)\n",
    "        \n",
    "        if (F is not None) and (g is not None):\n",
    "            while np.count_nonzero(g - F*(curr_x + eps * delta_x) <= 0) > 0: #projectioin\n",
    "                eps = 0.6 * eps\n",
    "        \n",
    "        s = eps * delta_x\n",
    "        y = grad(curr_x+s, **kwargs) - grad(curr_x, **kwargs)\n",
    "        \n",
    "        precision = np.max(np.absolute(s))\n",
    "        curr_x = curr_x + s\n",
    "        \n",
    "        if precision < stop_precision or np.all(grad(curr_x, **kwargs) == 0):\n",
    "            break\n",
    "        \n",
    "        inverse_hesse = update_inverse_hesse(inverse_hesse, s, y)\n",
    "        \n",
    "    return curr_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The following example shows the advantages of using barrier method instead of simple gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min at  [[2.01093455]\n",
      " [0.6593663 ]]\n"
     ]
    }
   ],
   "source": [
    "print('Min at ', grad_descent(grad_b, start_b, F= FF, g= gg, stop_precision=0.0001, epsilon=0.001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For gradient descent, max =  [[6.59437475]]\n",
      "For barrier method, max =  [[12.98826058]]\n"
     ]
    }
   ],
   "source": [
    "print('For gradient descent, max = ', -f_b(np.matrix([[2.01093455],\n",
    "        [0.6593663 ]])))\n",
    "print('For barrier method, max = ', -f_b(np.matrix([[ 7.55725553],\n",
    "     [-3.03861009]])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see, that using barrier method gives better results.\n",
    "\n",
    "\n",
    "### Newton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def newton_method(grad, hesse, start_x, F= None, g=None, \\\n",
    "                  stop_precision=0.0001, step_size=1, iterations=1000, **kwargs):\n",
    "    \"\"\"\n",
    "    Minimizes function using Newton method\n",
    "    grad - gradient of a function to minimize\n",
    "    start_x - starting point\n",
    "    \"\"\"\n",
    "    curr_x = start_x\n",
    "    for i in range(iterations):\n",
    "        eps = step_size\n",
    "        inverse_hesse = (hesse(curr_x, **kwargs))**(-1)\n",
    "        gradient = grad(curr_x, **kwargs)\n",
    "        \n",
    "        delta_x = - inverse_hesse * gradient\n",
    "        \n",
    "        if (F is not None) and (g is not None):\n",
    "            while np.count_nonzero(g - F*(curr_x + eps * delta_x) <= 0) > 0: #projectioin\n",
    "                eps = 0.6 * eps\n",
    "        \n",
    "        lambda_square = gradient.T * inverse_hesse * gradient\n",
    "        if lambda_square/2 < stop_precision:\n",
    "            return curr_x + eps * delta_x\n",
    "        curr_x = curr_x + eps*delta_x\n",
    "    return curr_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min at [[ 7.99593409]\n",
      " [-3.33164255]]\n"
     ]
    }
   ],
   "source": [
    "print('min at', barrier_method(start_b, 0.4, FF, gg, grad_b, hesse_func= hesse_b,\n",
    "               optimization_method= newton_method, mu= 10, iterations_limit = 100000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will check, if our methods work correctly for function without restrictions. As an example, we will use function $x^2 - 5x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad-descent min = -2.4501943322886444\n",
      "quasi-newton min = [[-2.5]]\n",
      "newton min = -2.5\n"
     ]
    }
   ],
   "source": [
    "def grad_square(x):\n",
    "    return 2*x + 5\n",
    "def hesse_square(x):\n",
    "    return 2\n",
    "\n",
    "print('grad-descent min =', grad_descent\\\n",
    "      (grad_square, np.array(0)))\n",
    "print('quasi-newton min =', quasi_newton_method\\\n",
    "      (grad_square, np.array(0), stop_precision=0.0001, step_size=1, iterations=10000))\n",
    "print('newton min =', newton_method\\\n",
    "      (grad_square, hesse_square,  np.array(0), iterations=10000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, all method work correctly.\n",
    "\n",
    "First let us examine 1-D example.\n",
    "$$ a = 1, b = -2, c = 2, d=2, F = 2, g = 15$$ Then the function is:\n",
    "a) $$ \\frac {exp(x-2)}{1+exp(x-2)}$$\n",
    "b) $$ \\frac {exp(x-2)}{1+exp(x-2)}(2x+2)$$\n",
    "\n",
    "subject to $2x<15$\n",
    "\n",
    "Because function is increasing, maximum will br at 7,5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max grad descent at  [[7.49825843]]\n",
      "Max quasi newton at  [[7.49767651]]\n",
      "Max newton at [[7.49794752]]\n"
     ]
    }
   ],
   "source": [
    "a = np.matrix([1]).T\n",
    "b = -2\n",
    "c = np.matrix([2]).T\n",
    "d = 2\n",
    "start_a = np.matrix([0]).T\n",
    "start_b = np.matrix([1]).T\n",
    "g = np.matrix([15]).T\n",
    "F = np.matrix([[2]])\n",
    "FF = np.concatenate((F, -c.T))\n",
    "gg = np.vstack((g, d))\n",
    "\n",
    "stop_precision = 0.001\n",
    "epsilon = 0.01\n",
    "print('Max grad descent at ', barrier_method(start_b, 0.4, \\\n",
    "        FF, gg, grad_b, mu= 10, iterations_limit = 100000, a=a, b=b, c=c, d=d))\n",
    "print('Max quasi newton at ', barrier_method(start_b, 0.4, FF, gg, grad_b, \n",
    "               optimization_method= quasi_newton_method, mu= 10, iterations_limit = 100000, a=a, b=b, c=c, d=d))\n",
    "print('Max newton at', barrier_method(start_b, 0.4, FF, gg, grad_b, hesse_func= hesse_b,\n",
    "               optimization_method=newton_method, mu= 10, iterations_limit = 100000, a=a, b=b, c=c, d=d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us check if they work correctly for our function. As an example we take $$ a = (1,2), b = 1, c = (2,1), d=2, F = ((2,3),(1,0)), g = (6,8)$$ Then the function is:\n",
    "a) $$ \\frac {exp(x_1 + 2x_2+1)}{1+exp(x_1 + 2x_2+1)}$$\n",
    "b) $$ \\frac {exp(x_1 + 2x_2+1)}{1+exp(x_1 + 2x_2+1)}(2x_1 + x_2 + 2)$$\n",
    "\n",
    "subject to $2x_1 + 3x_2 < 6$ and $x_1 < 8$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.matrix([1,2]).T\n",
    "b = 1\n",
    "c = np.matrix([2,1]).T\n",
    "d = 2\n",
    "start_a = np.matrix([1, 1]).T\n",
    "start_b = np.matrix([1, 0]).T\n",
    "g = np.matrix([6, 8]).T\n",
    "F = np.matrix([[2, 3],[1, 0]])\n",
    "\n",
    "FF = np.concatenate((F, -c.T))\n",
    "gg = np.vstack((g, d))\n",
    "\n",
    "stop_precision = 0.001\n",
    "epsilon = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, for a):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max grad descent a at  [[-3872.71308567]\n",
      " [ 2583.80842628]]\n",
      "Max =  [[1295.90376689]]\n"
     ]
    }
   ],
   "source": [
    "print('Max grad descent a at ', barrier_method(start_a, 0.4, \\\n",
    "        F, g, grad_a, mu= 10, iterations_limit = 100000, a=a))\n",
    "print('Max = ', -f_a([[-3872.71308567],\n",
    " [ 2583.80842628]],a=a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For b):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max grad descent at  [[ 7.55725553]\n",
      " [-3.03861009]]\n",
      "CPU times: user 2.12 s, sys: 4 ms, total: 2.13 s\n",
      "Wall time: 2.12 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print('Max grad descent at ', barrier_method(start_b, 0.4, \\\n",
    "        FF, gg, grad_b, mu= 10, iterations_limit = 100000, a=a, b=b, c=c, d=d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max quasi newton at  [[ 7.99688143]\n",
      " [-3.33365584]]\n",
      "CPU times: user 24 ms, sys: 0 ns, total: 24 ms\n",
      "Wall time: 22 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print('Max quasi newton at ', barrier_method(start_b, 0.4, FF, gg, grad_b, \n",
    "               optimization_method= quasi_newton_method, mu= 10, iterations_limit = 100000, a=a, b=b, c=c, d=d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max newton at [[ 7.99593409]\n",
      " [-3.33164255]]\n",
      "CPU times: user 8 ms, sys: 12 ms, total: 20 ms\n",
      "Wall time: 9.83 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print('Max newton at', barrier_method(start_b, 0.4, FF, gg, grad_b, hesse_func= hesse_b,\n",
    "               optimization_method=newton_method, mu= 10, iterations_limit = 100000, a=a, b=b, c=c, d=d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
