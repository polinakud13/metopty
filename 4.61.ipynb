{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem statement\n",
    "\n",
    "Let $X \\in \\{0,1\\}$ be a random variable, that satisfies $$prob(X=1) = p = \\frac {exp(a^{T}x+b)}{1+exp(a^{T}x+b)} $$  where $x \\in \\mathbb{R}^n$ is a vector of variables that affect the probability, and $a$ and $b$ are known parameters. We can think of $X = 1$ as the event that a consumer buys a product, and $x$ as a vector of variables that affect the probability, e.g., advertising effort, retail price, discounted price, packaging expense, and other factors. \n",
    "The variable $x$, which we are to optimize over, is subject to a set of linear constraints, $Fx \\preceq g$.\n",
    "\n",
    "Formulate the following problems as convex optimization problems.\n",
    "\n",
    "\n",
    "1)  _Maximizing buying probability_ The goal is to choose $x$ to maximize $p$.\n",
    "\n",
    "2)  _Maximizing expected profit._ Let $c^T x+d$ be the profit derived from selling the product, which we assume is positive for all feasible x. The goal is to maximize the expected profit, which is $p\\cdot(c^T x + d)$.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as sps\n",
    "import cvxpy as cvx\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_a(x):\n",
    "    global a\n",
    "    return a\n",
    "\n",
    "def grad_b(x):\n",
    "    global a,b,c,d\n",
    "    nom = np.exp((a*x).sum() + b)\n",
    "    denom = 1 + nom\n",
    "    return a - nom*a/denom + c/((c*x).sum() + d)\n",
    "\n",
    "def grad_descent(start_x, stop_precision, epsilon, grad):\n",
    "    work_x = start_x\n",
    "    prev_x = 0\n",
    "    prev_precision = 1\n",
    "    iterations = 0\n",
    "    while prev_precision > stop_precision and iterations < 100000:\n",
    "        prev_x = work_x\n",
    "        work_x = work_x - epsilon*grad(work_x)\n",
    "        prev_precision = np.max(np.absolute(prev_x - work_x))\n",
    "        iterations += 1\n",
    "    print(iterations)\n",
    "    return work_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1,2])\n",
    "b = 1\n",
    "c = np.array([2,1])\n",
    "d = 2\n",
    "start_a = [5, 3]\n",
    "start_b = [4, 3]\n",
    "stop_precision = 0.00001\n",
    "epsilon = 0.01"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
