{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem statement\n",
    "\n",
    "Let $X \\in \\{0,1\\}$ be a random variable, that satisfies $$prob(X=1) = p = \\frac {exp(a^{T}x+b)}{1+exp(a^{T}x+b)} $$  where $x \\in \\mathbb{R}^n$ is a vector of variables that affect the probability, and $a$ and $b$ are known parameters. We can think of $X = 1$ as the event that a consumer buys a product, and $x$ as a vector of variables that affect the probability, e.g., advertising effort, retail price, discounted price, packaging expense, and other factors. \n",
    "The variable $x$, which we are to optimize over, is subject to a set of linear constraints, $Fx \\preceq g$.\n",
    "\n",
    "Formulate the following problems as convex optimization problems.\n",
    "\n",
    "\n",
    "1)  _Maximizing buying probability_ The goal is to choose $x$ to maximize $p$.\n",
    "\n",
    "2)  _Maximizing expected profit._ Let $c^T x+d$ be the profit derived from selling the product, which we assume is positive for all feasible x. The goal is to maximize the expected profit, which is $p\\cdot(c^T x + d)$.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as sps\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.matrix([1,2]).T\n",
    "a\n",
    "b = 1\n",
    "c = np.matrix([2,1]).T\n",
    "d = 2\n",
    "start_a = np.matrix([5, 3]).T\n",
    "start_b = np.matrix([4, 3]).T\n",
    "stop_precision = 0.001\n",
    "epsilon = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_descent(grad, start_x, stop_precision=0.0001, epsilon=0.01):\n",
    "    work_x = start_x\n",
    "    prev_x = 0\n",
    "    prev_precision = 1\n",
    "    for i in range(1000):\n",
    "        prev_x = work_x\n",
    "        work_x = work_x - epsilon*grad(work_x)\n",
    "        prev_precision = np.max(np.absolute(prev_x - work_x))\n",
    "        if prev_precision < stop_precision:\n",
    "            break\n",
    "    return work_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_a(x):\n",
    "    return np.matrix(-a)\n",
    "\n",
    "def grad_b(x):\n",
    "    nom = np.exp(a.T*x + b)[0, 0]\n",
    "    denom = 1 + nom\n",
    "    if c.T*x + d <= 0:\n",
    "        raise Exception('grad_b error: Divided by zero')\n",
    "    return np.matrix(-a + nom*a/denom - c/(c.T*x + d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[5.36067174],\n",
       "        [3.68041794]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_descent(grad_b, start_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hesse matrix\n",
    "def hesse_a(x):\n",
    "    return 0\n",
    "def hesse_b(x):\n",
    "    nom = np.exp(a.T*x + b)[0,0]\n",
    "    if c.T*x + d <= 0:\n",
    "        raise Exception('grad_b error: Divided by zero')\n",
    "    return c*c.T/(c.T*x+d)**2 + nom/(nom+1)**2*a*a.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def newton_method(grad, hesse, start_x, stop_precision=0.000001, step_size=1, iterations=1000):\n",
    "    \"\"\"\n",
    "    Minimizes function using Newton method\n",
    "    grad - gradient of a function to minimize\n",
    "    start_x - starting point\n",
    "    \"\"\"\n",
    "    curr_x = start_x\n",
    "    for i in range(iterations):\n",
    "        inverse_hesse = hesse(curr_x).I\n",
    "        gradient = grad(curr_x)\n",
    "        \n",
    "        delta_x = - inverse_hesse * gradient\n",
    "        lambda_square = gradient.T * inverse_hesse * gradient\n",
    "        if lambda_square/2 < stop_precision:\n",
    "            print(i+1)\n",
    "            return curr_x + delta_x\n",
    "        curr_x = curr_x + delta_x\n",
    "    return curr_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quasi_newton_method(grad, start_x, stop_precision=0.0001, step_size=1, iterations=10000):\n",
    "    \"\"\"\n",
    "    Minimizes function using quasi Newton method\n",
    "    grad - gradient of a function to minimize\n",
    "    start_x - starting point\n",
    "    \"\"\"\n",
    "    def update_inverse_hesse(prev, s, y):\n",
    "        v = s - prev*y\n",
    "        if v.T*y == 0 or v.T*y != v.T*y:\n",
    "             raise Exception('hesse error: Divided by zero %f' % v.T*y)\n",
    "        return prev + v*v.T/(v.T*y)\n",
    "    \n",
    "    curr_x = start_x\n",
    "\n",
    "    inverse_hesse = np.identity(start_x.size) #hesse aproximation\n",
    "\n",
    "    for i in range(iterations): \n",
    "        delta_x = -inverse_hesse * grad(curr_x)\n",
    "        s = step_size * delta_x\n",
    "        y = grad(curr_x+s) - grad(curr_x)\n",
    "        \n",
    "        precision = np.max(np.absolute(s))\n",
    "        curr_x = curr_x + s\n",
    "        \n",
    "        if precision < stop_precision or grad(curr_x) == 0:\n",
    "            break\n",
    "        \n",
    "        inverse_hesse = update_inverse_hesse(inverse_hesse, s, y)\n",
    "        \n",
    "    return curr_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[-3.14152916]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quasi_newton_method(lambda x:np.matrix(x[0,0] + 3.1415291618), np.matrix(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "matrix([[0.00101496]])"
      ]
     },
     "execution_count": 386,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newton_method(lambda x:np.matrix(4*x[0,0]**3), lambda x: np.matrix(12*x[0,0]**2), np.matrix([1]),\n",
    "              iterations=100000, stop_precision=0.00000000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#quasi_newton_method(grad_b, start_x=start_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "def line_search(r, x, mu, z_p, z_d, c, beta):\n",
    "    alpha = 1\n",
    "    while norm(r(x + alpha * z_p, mu + alpha * z_d)) >= (1 - c * alpha) * norm(r(x, mu)): \n",
    "        alpha *= beta   \n",
    "    return alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#newton_method(grad_b, hesse_b, start_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
